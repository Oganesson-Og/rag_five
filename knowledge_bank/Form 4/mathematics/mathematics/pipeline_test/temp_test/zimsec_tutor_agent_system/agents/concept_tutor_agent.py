# concept_tutor_agent.py
"""
ZIMSEC Tutoring System - Concept Tutor Agent
--------------------------------------------

This module defines the `ConceptTutorAgent`, an AI agent specialized in
providing detailed explanations and tutoring on specific concepts identified
by the `OrchestratorAgent` within the ZIMSEC Tutoring System.

Key Features:
- Receives structured data (user query, syllabus alignment, RAG context) from Orchestrator.
- Uses RAG to fetch detailed knowledge content for the identified topic/subtopic.
- Constructs a prompt for its LLM, including the query, syllabus context, and RAG content.
- Instructs its LLM to provide clear, step-by-step explanations using Markdown.
- Aims for conversational and engaging responses, encouraging further student interaction.
- Instructs its LLM to return a JSON object containing the textual answer and a suggested image path if relevant.
- Parses the LLM's JSON response and forwards it to the Orchestrator.

Technical Details:
- Inherits from `autogen.AssistantAgent`.
- Defines a system message guiding the LLM on response style, content incorporation, and output format (JSON).
- Registers a custom reply function (`_generate_tutor_reply`) to handle interaction logic.
- Integrates with the `rag_integration` module to fetch knowledge content.

Dependencies:
- autogen
- json
- asyncio
- os
- logging
- ../rag_integration.py (get_knowledge_content_from_rag)

Author: Keith Satuku
Version: 1.0.0
Created: 2025
License: MIT
"""

import autogen
import json
import asyncio # Added for async function
import os
import logging # <--- Add this import

# Setup logger for this module
logger = logging.getLogger(__name__) # <--- Add this line

# Import the RAG integration function using absolute path
# from ..rag_integration import get_knowledge_content_from_rag # OLD
from zimsec_tutor_agent_system.rag_integration import get_knowledge_content_from_rag # NEW

class ConceptTutorAgent(autogen.AssistantAgent):
    """
    The ConceptTutorAgent is responsible for providing detailed explanations for
    specific concepts, guided by syllabus alignment data and retrieved knowledge content.

    It receives a JSON payload from the OrchestratorAgent containing the student's
    original query and rich contextual information (topic, subtopic, learning outcomes,
    mandatory terms, and form level). The agent then uses this context to fetch
    relevant information from a knowledge base via RAG (Retrieval Augmented Generation).

    The core task of this agent is to synthesize all this information into a clear,
    concise, and pedagogically sound explanation for the student. The explanation is
    generated by an LLM, which is prompted with the user query, syllabus details,
    and the retrieved RAG content. The LLM is instructed to format its output as a
    JSON object containing the textual answer and an optional path to a relevant image.

    The agent ensures that explanations are aligned with the ZIMSEC curriculum,
    incorporate mandatory terminology, and address specified learning outcomes.
    It aims for a conversational and supportive tone, encouraging student engagement.
    """
    def __init__(self, name, llm_config, **kwargs):
        # system_message = (
        #     "You are the Concept Tutor Agent in a multi-agent AI tutoring system for ZIMSEC O-Level students.\n"
        #     "You will receive a JSON message containing the user's original query and detailed syllabus alignment context from the Orchestrator Agent.\n"
        #     "Your primary goal is to provide a clear, step-by-step explanation for the user's query, grounded in the provided context.\n"
        #     "Follow these steps:\n"
        #     "1. Acknowledge the specific topic based on the context (e.g., 'Okay, let's look at finding the median for grouped data.').\n"
        #     "2. Use the provided syllabus outcomes and mandatory terms to guide your explanation.\n"
        #     "3. Incorporate the retrieved knowledge content (which will be provided in the prompt context) to explain the concept or procedure.\n"
        #     "4. Start with a simple definition or the first key step.\n"
        #     "5. Consider asking a brief Socratic question to engage the learner if appropriate (e.g., 'Before we use the formula, what does cumulative frequency tell us?').\n"
        #     "6. Present information clearly using Markdown formatting (bold terms, lists, code blocks for math if needed).\n"
        #     "7. Keep the explanation focused on the specific query and syllabus outcomes.\n"
        #     "8. Ensure you use the key `mandatory_terms` from the alignment data in your explanation.\n"
        #     "9. If the user's query is a direct request to 'calculate', 'find the value of', 'compute', or similar, and all necessary numerical inputs are provided or are simple to deduce from the context and retrieved knowledge, perform the calculation. Clearly show the method, the steps involved, and provide the final numerical answer as part of your response."
        # )
        system_message = (
            "You are the Concept Tutor Agent, assisting ZIMSEC O-Level students clearly and naturally, strictly aligned with the syllabus context provided.\n"
            "When responding:\n"
            "1. Start directly with a concise definition or explanation without explicitly restating the student's question or topic.\n"
            "2. Provide short, clear answers by default, explicitly using provided syllabus outcomes and mandatory terms.\n"
            "3. Naturally incorporate relevant retrieved knowledge content.\n"
            "4. Use conversational language with Markdown formatting (bold terms, bullet points, inline `code`, and clearly formatted equations).\n"
            "5. For calculation requests with sufficient data, concisely outline each step, clearly state the final answer, then politely ask if further explanation is needed.\n"
            "6. After your initial concise response, always encourage further engagement by asking if the student wants additional examples, more detail, or clarification.\n"
            "Maintain a supportive, approachable tone and avoid overly formal language."
        )

        super().__init__(name, system_message=system_message, llm_config=llm_config, **kwargs)
        
        self.register_reply(
            autogen.Agent, # Triggered by messages from other agents (Orchestrator)
            ConceptTutorAgent._generate_tutor_reply
        )

    # Remove or comment out the mock RAG function
    # def _mock_retrieve_knowledge_content(self, topic: str, subtopic: str) -> str:
    #     # ... mock implementation ...

    async def _generate_tutor_reply(self, messages, sender, config):
        """
        Generates a detailed, syllabus-aligned explanation for a student's query.

        This asynchronous method is the primary handler for messages received by the
        `ConceptTutorAgent`, typically from the `OrchestratorAgent`.

        Steps:
        1.  Parses the incoming JSON message to extract the user's original query,
            syllabus alignment data (topic, subtopic, form, outcomes, mandatory terms),
            and any other relevant context.
        2.  Uses the identified topic, subtopic, and form to retrieve detailed knowledge
            content from the RAG (Retrieval Augmented Generation) system via
            `get_knowledge_content_from_rag`.
        3.  Constructs a comprehensive prompt for its internal LLM. This prompt includes:
            - The agent's system message (guiding its persona and task).
            - The student's original query.
            - The structured syllabus alignment details.
            - The retrieved knowledge content from RAG.
            - Specific instructions for the LLM to return its response as a JSON object
              with keys for 'answer_text' (the textual explanation) and
              'suggested_image_path' (an optional path to a relevant diagram).
        4.  Calls the LLM (via `self.a_generate_oai_reply`) with the constructed prompt.
        5.  Parses the LLM's JSON response.
            - Extracts the `answer_text` and `suggested_image_path`.
            - Handles potential JSON decoding errors or missing keys gracefully.
        6.  Packages the `answer_text`, the `retrieved_rag_context` (for transparency or downstream use),
            and the `suggested_image_path` into a final JSON payload.
        7.  Returns this JSON payload to the calling agent (Orchestrator).

        Args:
            messages (List[Dict]): A list of messages. The last message is expected to be
                                 a JSON string from the OrchestratorAgent containing
                                 the necessary context.
            sender (autogen.Agent): The agent that sent the message (typically OrchestratorAgent).
            config (Any, optional): Configuration data, not actively used in this method.

        Returns:
            Tuple[bool, str]: A tuple where the first element is a boolean indicating
                              success (True), and the second element is a JSON string
                              representing the tutor's structured response, ready to be
                              passed back to the OrchestratorAgent.
                              If critical errors occur (e.g., JSON parsing of input),
                              it may return a simple error string with TERMINATE.
        """
        last_message = messages[-1]
        message_content = last_message.get("content", "")

        try:
            orchestrator_data = json.loads(message_content)
            user_query = orchestrator_data.get("original_user_query", "")
            alignment_data = orchestrator_data.get("alignment_data", {})
            logger.debug(f"\n ConceptTutor: Received data from Orchestrator for query '{user_query}'")
        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"\n ConceptTutor: Error decoding JSON from Orchestrator: {e}")
            return True, "I received the request, but had trouble understanding the details... TERMINATE"

        topic = alignment_data.get("identified_topic", "Unknown Topic")
        subtopic = alignment_data.get("identified_subtopic", "Unknown Subtopic")
        # Get the form from alignment data, crucial for knowledge retrieval
        form = alignment_data.get("identified_form", "Form 4") # Default to Form 4 if not present
        outcomes = alignment_data.get("matched_outcomes", [])
        mandatory_terms = alignment_data.get("mandatory_terms", [])

        # Retrieve knowledge content using actual RAG function
        logger.debug(f"\n ConceptTutor: Retrieving knowledge using RAG for Topic='{topic}', Subtopic='{subtopic}', Form='{form}'")
        retrieved_content_str = get_knowledge_content_from_rag(topic, subtopic, form)

        # This is crucial: if RAG fails or returns no content, we should still inform the LLM.
        if not retrieved_content_str: # Check if the string is empty or None
            retrieved_content_str = "No specific knowledge content was retrieved for this topic. Please answer based on general knowledge if possible, or indicate that you cannot provide a specific explanation without more information."
            logger.warning(f"No RAG content retrieved for Topic='{topic}', Subtopic='{subtopic}', Form='{form}'")
        else:
            logger.debug(f"RAG content prepared for LLM. Snippet: {retrieved_content_str[:200]}...")

        # Construct a new prompt for the LLM containing all context
        # Note: This message history is internal to this agent's LLM call
        internal_messages = [
            {"role": "system", "content": self.system_message},
            {
                "role": "user",
                "content": (
                    f"I have the following student query: '{user_query}'.\n\n"
                    f"Syllabus details:\n"
                    f"- Topic: {topic}\n"
                    f"- Subtopic: {subtopic}\n"
                    f"- Relevant Outcomes: {', '.join(outcomes) if outcomes else 'N/A'}\n"
                    f"- Mandatory Terms: {', '.join(mandatory_terms) if mandatory_terms else 'N/A'}\n\n"
                    f"Retrieved relevant content:\n---\n{retrieved_content_str}\n---\n\n"
                    "Your response MUST be a JSON object with the following keys: 'answer_text' (string) and 'suggested_image_path' (string or null).\n"
                    "- 'answer_text': This is the textual explanation for the student. It should be short, precise, and clearly anchored to the syllabus details and retrieved content. Use conversational language and Markdown formatting.\n"
                    "- 'suggested_image_path': If the 'Retrieved relevant content' includes an 'Image Path' AND you determine that this image is directly helpful and relevant to the explanation, set this key to the 'Image Path' value from the content (e.g., \"math_diagrams/image.png\"). Otherwise, set it to null.\n\n"
                    "Guidelines for 'answer_text':\n"
                    "1. If an image is being suggested via 'suggested_image_path' (i.e., it's not null), you can briefly mention that a diagram is available and describe it using the 'Image Description' from the retrieved content.\n"
                    "2. If the student's query involves calculations with sufficient data, include the step-by-step calculation and final answer within the 'answer_text'.\n"
                    "3. After the main explanation in 'answer_text', ALWAYS politely ask if the student would like a more detailed explanation, additional examples, or clarification.\n"
                )
            }
        ]

        
        # Generate the reply using the AssistantAgent's standard method
        # This uses the llm_config passed during initialization
        logger.debug(f"\n ConceptTutor: Generating LLM reply based on context...")
        # Use the agent's inherited method to generate reply based on internal messages
        # This requires access to the client, often done via self.client
        success, reply_obj = await self.a_generate_oai_reply(internal_messages)

        llm_output = {}
        final_answer_text = "Sorry, I encountered an issue while formulating my response."
        suggested_image_path = None

        if success and reply_obj:
            llm_response_content = reply_obj if isinstance(reply_obj, str) else reply_obj.get("content")
            
            if llm_response_content:
                try:
                    # Attempt to strip ```json ... ``` markdown if present
                    if llm_response_content.strip().startswith("```json"):
                        llm_response_content = llm_response_content.strip()[7:] # Length of "```json\n"
                        if llm_response_content.strip().endswith("```"):
                             llm_response_content = llm_response_content.strip()[:-3]
                    
                    llm_output = json.loads(llm_response_content.strip()) # Added strip() for robustness
                    final_answer_text = llm_output.get("answer_text", "I couldn't quite generate the text for that, sorry!")
                    suggested_image_path = llm_output.get("suggested_image_path")
                    if isinstance(suggested_image_path, str) and not suggested_image_path.strip():
                        suggested_image_path = None # Treat empty string as None
                    logger.debug(f"ConceptTutor LLM successfully parsed JSON output. Image suggested: {suggested_image_path}")

                except json.JSONDecodeError as e:
                    logger.warning(f"ConceptTutor LLM did not return valid JSON. Error: {e}. Raw reply: {llm_response_content}")
                    final_answer_text = llm_response_content 
                    suggested_image_path = None 
            else:
                logger.warning(f"ConceptTutor LLM reply content was empty. Reply object: {reply_obj}")
        else:
            logger.error(f"ConceptTutor LLM call failed or returned empty. Success: {success}, Reply Obj: {reply_obj}")

        # Package the payload
        final_payload = {
            "answer": final_answer_text,
            "retrieved_rag_context": retrieved_content_str, 
            "suggested_image_path": suggested_image_path
        }
        return True, json.dumps(final_payload)

if __name__ == '__main__':
    # Example for testing the agent in isolation
    config_list_test = [
        {
            "model": "gpt-4.1-nano",
            "api_key": os.environ.get("OPENAI_API_KEY")
        }
    ]

    concept_tutor = ConceptTutorAgent(
        name="ConceptTutorTest",
        llm_config={"config_list": config_list_test}
    )

    # Mock message from Orchestrator
    mock_orchestrator_message = {
        "original_user_query": "Tell me about the median from a grouped frequency table.",
        "alignment_data": {
            "is_in_syllabus": True,
            "alignment_score": 0.95,
            "matched_outcomes": ["F4-Stats-MedianGrouped-Obj1", "F4-Stats-CFcurve-Obj3"],
            "mandatory_terms": ["median class", "cumulative frequency", "interpolation", "lower class boundary"],
            "identified_subject": "Mathematics",
            "identified_topic": "Statistics",
            "identified_subtopic": "Measures of Central Tendency (Grouped Data)",
            "identified_form": "Form 4" # Ensure form is part of alignment_data for RAG
        }
    }

    # Simulate receiving message (content should be structured)
    # We might need the LLM to process this structured input based on the system prompt
    logger.debug("--- Testing Concept Tutor Agent (LLM Reply) ---")
    # Use a dummy sender for the test
    dummy_sender = autogen.Agent(name="DummyOrchestrator")
    
    # Autogen works best with string messages, so pass JSON as string
    # This is an async function, so we need to run it in an event loop for testing
    async def run_concept_tutor_test():
        success_test, reply_test = await concept_tutor._generate_tutor_reply(
            messages=[{"role": "user", "content": json.dumps(mock_orchestrator_message)}],
            sender=dummy_sender,
            config=None
        )
        if success_test:
            logger.info(f"Concept Tutor Test Reply:\n{reply_test}")
        else:
            logger.error("Concept Tutor Test Failed.")

    if os.environ.get("OPENAI_API_KEY"):
         asyncio.run(run_concept_tutor_test())
    else:
        logger.warning("OPENAI_API_KEY not set. Skipping ConceptTutorAgent __main__ test.")
    
    logger.debug("Concept Tutor agent created. Test execution in isolation needs refinement or run via main.py.") 